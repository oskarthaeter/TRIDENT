{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial 3: Training a WSI Classification Model with ABMIL\n",
    "\n",
    "This tutorial will guide you step-by-step to train an attention-based multiple instance learning model using Trident patch embeddings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A- Installation and patch feature extraction using UNI\n",
    "\n",
    "#### Step 1: Download a dataset of whole-slide images\n",
    "\n",
    "You can use your own WSIs or download a publicly available dataset, e.g. from:\n",
    "\n",
    "- **CPTAC CCRCC WSIs**: Download from the [TCIA Cancer Imaging Archive](https://www.cancerimagingarchive.net/collection/cptac-ccrcc/).\n",
    "- **Storage**: Save all WSIs into a local directory, e.g.,  \n",
    "  ```bash\n",
    "  ./CPTAC-CCRCC_v1/CCRCC\n",
    "  ```\n",
    "\n",
    "#### Step 2:  Run UNI feature extraction:\n",
    "\n",
    "Navigate to the base directory of Trident and execute the following command:\n",
    "\n",
    "```bash\n",
    "python run_batch_of_slides.py --task all \\\n",
    "  --wsi_dir ./CPTAC-CCRCC_v1/CCRCC \\\n",
    "  --job_dir ./trident_processed \\\n",
    "  --patch_encoder uni_v1 \\\n",
    "  --mag 20 \\\n",
    "  --patch_size 256\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B- Training an ABMIL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Download labels as csv\n",
    "datasets.load_dataset(\n",
    "    'MahmoodLab/Patho-Bench', \n",
    "    cache_dir='./tutorial-3',\n",
    "    dataset_to_download='cptac_ccrcc',     \n",
    "    task_in_dataset='BAP1_mutation',           \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Visualize my labels and splits\n",
    "df = pd.read_csv('tutorial-3/cptac_ccrcc/BAP1_mutation/k=all.tsv', sep=\"\\t\")\n",
    "print(df.value_counts('BAP1_mutation'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from trident.slide_encoder_models.load import ABMILSlideEncoder\n",
    "\n",
    "# Build binary classification model\n",
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self, input_feature_dim=768, n_heads=1, head_dim=512, dropout=0., gated=True, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.feature_encoder = ABMILSlideEncoder(\n",
    "            input_feature_dim=input_feature_dim, \n",
    "            n_heads=n_heads, \n",
    "            head_dim=head_dim, \n",
    "            dropout=dropout, \n",
    "            gated=gated\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_feature_dim, hidden_dim),  # head_dim from ABMILSlideEncoder output\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_encoder(x)  # Encode features\n",
    "        logits = self.classifier(features).squeeze(1)  # Output logits\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = BinaryClassificationModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Custom dataset\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, feats_path, df, split, num_features=512):\n",
    "        self.df = df[df[\"fold_1\"] == split]\n",
    "        self.feats_path = feats_path\n",
    "        self.num_features = num_features\n",
    "        self.split = split\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        features = h5py.File(os.path.join(self.feats_path, row['slide_id'] + '.h5'), \"r\")  \n",
    "        features = torch.from_numpy(features[\"features\"][:])\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            num_available = features.shape[0]\n",
    "            if num_available >= self.num_features:\n",
    "                indices = np.random.choice(num_available, self.num_features, replace=False)\n",
    "            else:\n",
    "                indices = np.random.choice(num_available, self.num_features, replace=True)  # Oversampling\n",
    "            features = features[indices]\n",
    "        \n",
    "        label = torch.tensor(row[\"BAP1_mutation\"], dtype=torch.float32)\n",
    "        return features, label\n",
    "\n",
    "# Create dataloaders\n",
    "feats_path = './cptac_ccrcc/20x_512px_0px_overlap/features_conch_v15'\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(H5Dataset(feats_path, df, \"train\"), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(H5Dataset(feats_path, df, \"test\"), batch_size=1, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-4)\n",
    "\n",
    "# Training loop on Fold 0. \n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = {'features': features.to(device)}, labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_labels, all_outputs = [], []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = {'features': features.to(device)}, labels.to(device)\n",
    "        outputs = model(features)\n",
    "        \n",
    "        # Convert logits to probabilities and binary predictions\n",
    "        predicted = (outputs > 0).float()  # Since BCEWithLogitsLoss expects raw logits\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        all_outputs.append(outputs.cpu().numpy())  \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        print(f\"Logits: {outputs.item():.4f}, Label: {labels.item()}\")\n",
    "\n",
    "# Compute AUC\n",
    "all_outputs = np.concatenate(all_outputs)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "auc = roc_auc_score(all_labels, all_outputs)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Test AUC: {auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trident",
   "language": "python",
   "name": "trident"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
