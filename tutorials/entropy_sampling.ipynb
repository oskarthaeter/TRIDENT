{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f279b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     11\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m/workspaces/TRIDENT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mextract_patches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m random_sample, entropy_bin_sampling, entropy_top_sampling\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TRIDENT/extract_patches.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroi_align\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[39m, in \u001b[36mregister_meta.<locals>.wrapper\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(fn):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension\u001b[49m._has_ops():\n\u001b[32m     19\u001b[39m         get_meta_lib().impl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch.ops.torchvision, op_name), overload_name), fn)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import openslide\n",
    "import tifffile\n",
    "import PIL.Image\n",
    "import h5py\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/workspaces/TRIDENT\")\n",
    "\n",
    "from extract_patches import random_sample, entropy_bin_sampling, entropy_top_sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96554d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_root_image_size(tif_path: Path) -> tuple[int, int]:\n",
    "    with tifffile.TiffFile(tif_path) as tif:\n",
    "        return (\n",
    "            tif.pages[0].tags[\"ImageWidth\"].value,\n",
    "            tif.pages[0].tags[\"ImageLength\"].value,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_example = Path(\"/mnt/nfs03-R6/staining/TCGA_256_40/TCGA-DB-A4XB-01Z-00-DX1.FBF60AEC-EF2E-4771-A46C-7ABF60C99D9C.svs\")\n",
    "h5_dir = Path(\"/mnt/nfs03-R6/staining/trident/40x_512px_0px_overlap/patches/\")\n",
    "h5_example = h5_dir / tcga_example.name.replace(\".svs\", \"_patches.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f729c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tiff tags to txt file\n",
    "def write_tiff_tags_to_txt(tiff_path, output_txt_path):\n",
    "    with tifffile.TiffFile(tiff_path) as tif:\n",
    "        with open(output_txt_path, 'w') as f:\n",
    "            for page in tif.pages:\n",
    "                tags = page.tags\n",
    "                f.write(f\"Page {page.index}:\\n\")\n",
    "                for tag in tags.values():\n",
    "                    if tag.name in {\"TileByteCounts\", \"TileOffsets\"}:\n",
    "                        # convert to list of integers\n",
    "                        # truncate to first 10 values for readability\n",
    "                        tag_value = list(tag.value)\n",
    "                        number_of_values = len(tag_value)\n",
    "                        if number_of_values > 10:\n",
    "                            tag_value = tag_value[:10]\n",
    "                        f.write(f\"{tag.name} ({number_of_values} values): {tag_value}...\\n\")\n",
    "                    elif tag.name in {\"InterColorProfile\", \"JPEGTables\"}:\n",
    "                        # write the first 10 bytes of the binary data\n",
    "                        tag_value = tag.value[:10]\n",
    "                        f.write(f\"{tag.name} (first 10 bytes): {tag_value}...\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"{tag.name}: {tag.value}\\n\")\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "# Example usage\n",
    "write_tiff_tags_to_txt(tcga_example, Path(\"/workspaces/TRIDENT\") / tcga_example.with_suffix('.txt').name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnail(svs_path: Path):\n",
    "    \"\"\"\n",
    "    Get the thumbnail of a slide.\n",
    "    \"\"\"\n",
    "    slide = openslide.open_slide(str(svs_path))\n",
    "    slide_width, slide_length = extract_root_image_size(svs_path)\n",
    "    patch_size = 512\n",
    "    edge_size = max(slide_width, slide_length) / patch_size\n",
    "    thumbnail = slide.get_thumbnail((edge_size, edge_size))\n",
    "    return thumbnail\n",
    "\n",
    "def visualize_coords(svs_path, coords):\n",
    "    \"\"\"\n",
    "    Visualize the coordinates on the thumbnail.\n",
    "    \"\"\"\n",
    "    thumbnail = get_thumbnail(svs_path)\n",
    "    # we need to the coords to the thumbnail size\n",
    "    slide_width, slide_length = extract_root_image_size(svs_path)\n",
    "    thumbnail_width, thumbnail_length = thumbnail.size\n",
    "    coords = [(int(x * thumbnail_width / slide_width), int(y * thumbnail_length / slide_length)) for x, y in coords]\n",
    "    # create a new image with the thumbnail\n",
    "    new_image = PIL.Image.new(\"RGB\", thumbnail.size)\n",
    "    new_image.paste(thumbnail, (0, 0))\n",
    "    # draw the coords\n",
    "    for x, y in coords:\n",
    "        new_image.putpixel((x, y), (0, 255, 0))\n",
    "    return new_image\n",
    "\n",
    "def visualize_patches(svs_path, coords):\n",
    "    \"\"\"\n",
    "    Visualize the patches on the thumbnail.\n",
    "    \"\"\"\n",
    "    slide = openslide.open_slide(str(svs_path))\n",
    "    patches = []\n",
    "    for x, y in coords:\n",
    "        # get the patch\n",
    "        patch = slide.read_region((x, y), 0, (512, 512))\n",
    "        patch = patch.resize((32, 32))\n",
    "        patches.append(patch)\n",
    "    # create one big image from the patches\n",
    "    # should be roughly square\n",
    "    num_patches = len(patches)\n",
    "    num_cols = int(np.sqrt(num_patches))\n",
    "    num_rows = int(np.ceil(num_patches / num_cols))\n",
    "    patch_width, patch_length = patches[0].size\n",
    "    new_image = PIL.Image.new(\"RGB\", (num_cols * patch_width, num_rows * patch_length))\n",
    "    for i, patch in enumerate(patches):\n",
    "        x = i % num_cols\n",
    "        y = i // num_cols\n",
    "        new_image.paste(patch, (x * patch_width, y * patch_length))\n",
    "    return new_image\n",
    "\n",
    "    \n",
    "entropy_coords = entropy_bin_sampling(num_bins=10, h5_path=h5_example, num_samples=1000, ignore_k_bins=2)\n",
    "print(f\"Entropy coords: {entropy_coords.shape}\")\n",
    "entropy_thumbnail = visualize_coords(tcga_example, entropy_coords)\n",
    "entropy_patches = visualize_patches(tcga_example, entropy_coords)\n",
    "\n",
    "entropy_binned_top_coords = entropy_top_sampling(num_bins=10, h5_path=h5_example, num_samples=1000, ignore_k_bins=2)\n",
    "print(f\"Entropy top coords: {entropy_binned_top_coords.shape}\")\n",
    "entropy_binned_top_thumbnail = visualize_coords(tcga_example, entropy_binned_top_coords)\n",
    "entropy_binned_top_patches = visualize_patches(tcga_example, entropy_binned_top_coords)\n",
    "\n",
    "entropy_top_coords = entropy_top_sampling(num_bins=1, h5_path=h5_example, num_samples=1000, ignore_k_bins=0)\n",
    "print(f\"Entropy top coords: {entropy_top_coords.shape}\")\n",
    "entropy_top_thumbnail = visualize_coords(tcga_example, entropy_top_coords)\n",
    "entropy_top_patches = visualize_patches(tcga_example, entropy_top_coords)\n",
    "\n",
    "random_coords = random_sample(h5_path=h5_example, num_samples=1000)\n",
    "print(f\"Random coords: {random_coords.shape}\")\n",
    "random_thumbnail = visualize_coords(tcga_example, random_coords)\n",
    "random_patches = visualize_patches(tcga_example, random_coords)\n",
    "\n",
    "# show the two thumbnails\n",
    "fig, axs = plt.subplots(4, 2, figsize=(20, 20))\n",
    "axs[0, 0].imshow(entropy_thumbnail)\n",
    "axs[0, 0].set_title(\"Entropy\")\n",
    "axs[0, 1].imshow(entropy_patches)\n",
    "axs[0, 1].set_title(\"Entropy Patches\")\n",
    "\n",
    "axs[1, 0].imshow(entropy_binned_top_thumbnail)\n",
    "axs[1, 0].set_title(\"Entropy binned Top\")\n",
    "axs[1, 1].imshow(entropy_binned_top_patches)\n",
    "axs[1, 1].set_title(\"Entropy binned Top Patches\")\n",
    "\n",
    "axs[2, 0].imshow(entropy_top_thumbnail)\n",
    "axs[2, 0].set_title(\"Entropy Top\")\n",
    "axs[2, 1].imshow(entropy_top_patches)\n",
    "axs[2, 1].set_title(\"Entropy Top Patches\")\n",
    "\n",
    "axs[3, 0].imshow(random_thumbnail)\n",
    "axs[3, 0].set_title(\"Random\")\n",
    "axs[3, 1].imshow(random_patches)\n",
    "axs[3, 1].set_title(\"Random Patches\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy_distribution(\n",
    "    h5_path: Path, sampled_coords: torch.Tensor = None, title: str = \"Entropy Distribution\"\n",
    ") -> torch.Tensor:\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        coords = torch.tensor(f[\"coords\"][:], dtype=torch.long)\n",
    "        bytecounts = torch.tensor(f[\"bytecounts\"][:], dtype=torch.float)\n",
    "    if bytecounts.shape[0] != coords.shape[0]:\n",
    "        raise ValueError(\"coords and bytecounts must have the same length\")\n",
    "    if bytecounts.ndim > 1:\n",
    "        bytecounts = torch.mean(bytecounts, dim=1)\n",
    "    \n",
    "    # get the indices of the sampled coords\n",
    "    if sampled_coords is not None:\n",
    "        sampled_coords = torch.tensor(sampled_coords, dtype=torch.long)\n",
    "        indices = []\n",
    "        for coord in sampled_coords:\n",
    "            index = torch.where(\n",
    "                (coords[:, 0] == coord[0]) & (coords[:, 1] == coord[1])\n",
    "            )[0]\n",
    "            if index.shape[0] > 0:\n",
    "                indices.append(index[0])\n",
    "        bytecounts = bytecounts[indices]\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.hist(bytecounts, bins=100, density=True)\n",
    "    ax.set_xlabel(\"Entropy\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_entropy_distribution(h5_example, title=\"Entropy Distribution of all tissue patches\")\n",
    "plot_entropy_distribution(h5_example, sampled_coords=entropy_coords, title=\"Entropy Distribution of entropy sampled patches\")\n",
    "plot_entropy_distribution(h5_example, sampled_coords=entropy_binned_top_coords, title=\"Entropy Distribution of entropy binned top sampled patches\")\n",
    "plot_entropy_distribution(h5_example, sampled_coords=entropy_top_coords, title=\"Entropy Distribution of entropy top sampled patches\")\n",
    "plot_entropy_distribution(h5_example, sampled_coords=random_coords, title=\"Entropy Distribution of random sampled patches\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
